# 아파치 카프카
다양한 메세지 플랫폼과 ETL 툴을 써서 아키텍처를 변경하고 데이터 파이프라인의 파편화 문제를 해결하려고 했지만, 링크드인 백엔드 개발팀은 해결하지 못했다.
그래서 새로운 신규 시스템을 만들기로 결정했고, 그게 바로 카프카이다.
소스 애플리케이션에 생성되는 데이터는 어느 타겟 어플리케이션으로 보낼지 신경쓰지 않고 카프카에 넣으면 된다.
카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 와 유사하다.

카프카를 통해서 전달할 수 있는 데이터의 포맷은 제한이 없다.

빅데이터를 저장하고 활용하려면, 생성되는 데이터를 모두 모으는 게 중요하다. 이 때 사용되는 개념이 데이터 레이크이다.
서비스의 수가 많아지고, 트래픽이 증가할 수록 이 모든 데이터를 end-to-end 로 모으는 것은 너무 비효율적이다.
그래서 데이터를 추출, 변경, 적재하는 과정을 묶은 데이터 파이프라인을 구축해야 하는 것이다.

## 카프카의 장점
### 높은 처리량
많은 양의 데이터를 주고 받을 때, 맺어지는 네트워크 비용은 절대 무시할 수 없다.
만약에 동일한 양의 데이터를 주고 받는데, 네트워크 통신 횟수가 적다면 이는 동일한 시간에 더 많은 데이터를 보낼 수 있다는 뜻이 된다.
또 파티션 단위로 동일 데이터를 분배하고 데이터를 병렬 처리할 수 있다. 컨슈머의 개수를 늘려서 데이터 처리량을 높일 수 있다.

### 확장성
데이터가 적을 때, 카프카 클러스터의 브로커를 최소한의 개수로 운영, 많아지면 다시 늘리는 방식으로 운영할 수 있다.
이는 예측하지 못한 비즈니스 상황에서 장점이다. 이 스케일 아웃, 스케일 인 과정이 무중단으로 가능하기 때문에 이커머스, 금융 등의 모델에서도 안정적으로 운영 가능하다.

### 영속성
데이터를 메모리에 저장하는 것이 아니라 파일 시스템에 저장한다. 그래서 갑자기 종료되어도 데이터를 보존할 수가 있다.

### 고가용성
3개 이상의 서버로 운영되는 카프카 클러스터는 데이터 복제가 가능하고, 서버 중단이 생겨도 무중단으로 안전하게 데이터를 운영할 수 있다.

#### 왜 최소 3개의 서버가 필요할까?
1개 : 서버가 죽으면 끝난다.
2개 : 하나가 죽어도 다른 한개가 돌아가서 괜찮지만, 브로커 간에 데이터 복제 시간 차이가 이어서 데이터 유실이 발생할 수 있다.
그러면 이 유실을 막으려면, min.insync.replicas 옵션을 2로 설정하면 되는데, 그러면 최소 2개 이상의 브로커에는 데이터 복제를 보장하는 것이다.
그러면 서버가 3개 여야 한다.

### 데이터 아키텍처
1. 람다 : end-to-end 로 데이터를 배치로 모았다. 
    - 배치 레이어 : 특정 시간, 타이밍마다 데이터를 일괄 처리한다.
    - 서빙 레이어 : 서비스 어플리케이션이 사용할 수 있도록 데이터 저장
    - 스피드 레이어 : 서비스에서 발생하는 데이터를 실시간으로 분석
-> 이렇게 레이어를 나누면, 데이터 분석, 처리의 과정이 융합되어서 유연하지 못한 파이프라인이 생성된다.
2. 카파 : 배치 레이어를 제거하고 모두 다 스피드 레이어에 넣는다. 그리고 여기서 다시 서빙 레이어(for people in data-related positions)를 제거한다.
    - 그냥 카프카에서 처리된 데이터를 조금 오랫동안 가지고 있으면 서빙 레이어가 굳이 따로 있을 필요가 없어진다. 그리고 카프카는 그렇게 해준다.